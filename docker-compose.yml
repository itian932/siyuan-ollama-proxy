version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # 关键：指定运行时为 nvidia (Docker Desktop 会自动识别)
    runtime: nvidia
    shm_size: "8gb"
    gpus: all
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1

  # 代理：负责给请求补上 model，并转发到 ollama 的 /v1
  proxy:
    build: ./proxy
    container_name: proxy
    restart: unless-stopped
    environment:
      - UPSTREAM_BASE=http://ollama:11434
      - DEFAULT_MODEL=llama3.1:latest     # 可以指定默认模型
      - AUTO_PULL=1
    ports:
      - "18000:18000"
    depends_on:
      - ollama

  # 思源：你可以按自己现有部署方式调整（端口、鉴权等）
  siyuan:
    image: apkdv/siyuan-unlock:latest
    container_name: siyuan
    restart: unless-stopped
    command: ['--workspace=/siyuan/workspace/', '--accessAuthCode=mtt2001']
    ports:
      - "6806:6806"
    volumes:
      - siyuan_data:/siyuan/workspace
    environment:
      - SIYUAN_PORT=6806
      - LANG=zh_CN.UTF-8
      - LC_ALL=zh_CN.UTF-8
    depends_on:
      - proxy

volumes:
  ollama_data:
  siyuan_data:
